{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d0cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from datasketch import MinHash,MinHashLSH\n",
    "from datasketch import MinHashLSHForest,MinHashLSHEnsemble\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter,defaultdict\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.utils import resample\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823d3854",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'TVs-all-merged.json' \n",
    "import json\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "# Extract the titles and model IDs for all products in the dataset\n",
    "product_titles = []\n",
    "model_ids = []\n",
    "for products in data.values():\n",
    "    for product in products:\n",
    "        product_titles.append(product['title'])\n",
    "        model_ids.append(product['modelID'])\n",
    "\n",
    "# Map the modelID to title (considering multiple instances)\n",
    "original_title_to_modelid = dict(zip(product_titles, model_ids))\n",
    "\n",
    "# Use indices as keys to maintain uniqueness\n",
    "index_to_modelid = {idx: model_id for idx, model_id in enumerate(model_ids)}\n",
    "    \n",
    "print(len(model_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b1fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the terms mentioned by MSMP+ paper \n",
    "# all spaces and non-alphanumeric tokens in front of the units are removed\n",
    "# all upper case is transformed into lower case\n",
    "def normalize_terms(text):\n",
    "    # Define patterns for 'inch' and 'hz' with possible preceding non-alphanumeric characters or spaces\n",
    "    inch_patterns = [r'\\s*[\\W_]*inch', r'\\s*[\\W_]*inches', r'\\s*[\\W_]*â€', r'\\s*[\\W_]*-inch', r'\\s*[\\W_]* inch']\n",
    "    hz_patterns = [r'\\s*[\\W_]*Hertz', r'\\s*[\\W_]*hertz', r'\\s*[\\W_]*Hz', r'\\s*[\\W_]*HZ', r'\\s*[\\W_]* hz', r'\\s*[\\W_]*-hz', r'\\s*[\\W_]*hz']\n",
    "    \n",
    "    # Replace with normalized terms\n",
    "    for pattern in inch_patterns:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    for pattern in hz_patterns:\n",
    "        text = re.sub(pattern, 'hz', text)\n",
    "        \n",
    "    return text\n",
    "\n",
    "def extract_features(features_map):\n",
    "    desired_features = ['Refresh Rate', 'Brand', \"Recommended Resolution\", \"Screen Size\", \"Screen Size Class\", \"Screen Refresh Rate\", \"Vertical Resolution\", \"Maximum Resolution\", \"Display Size\"]\n",
    "\n",
    "    # Convert desired features to lowercase for case-insensitive comparison\n",
    "    desired_features_lower = [feature.lower() for feature in desired_features]\n",
    "\n",
    "    # Convert the keys of features_map to lowercase\n",
    "    features_map_lower = {key.lower(): value for key, value in features_map.items()}\n",
    "\n",
    "    # Extract only the desired features, removing \"hz\" if the value ends with it\n",
    "    features_str = ' '.join([\n",
    "        features_map_lower.get(feature, '').rstrip('hz').rstrip() if feature in features_map_lower else ''\n",
    "        for feature in desired_features_lower\n",
    "    ])\n",
    "\n",
    "    # features_str = features_map\n",
    "    return features_str\n",
    "\n",
    "\n",
    "stop_words = set([\"yes\", \"no\", \"class\", \"best\", \"buy\", \"we\", \"it\", \"refurbished\", \"to\", \"a\", \"neweggcom\", \"tv\", \"smart\", \"series\", \"model\", \"tvs\", \"with\", \"apps\", \"x\"])  # Add more words as needed\n",
    "\n",
    "def preprocess_title(title, features_map):\n",
    "\n",
    "    # title = title + ' ' + extract_features(features_map)\n",
    "    # for x in range(10):\n",
    "    #     title = title + ' ' + (model_id+ str(x))\n",
    "    # Preprocess the title\n",
    "    title = title.lower()\n",
    "    title = re.sub(r'[^\\w\\s]', '', title)\n",
    "    title = title.replace('hdtv', 'hd')\n",
    "    title = title.replace('ledlcd', 'led lcd')\n",
    "\n",
    "    # Normalize specific terms\n",
    "    title = normalize_terms(title)\n",
    "\n",
    "    combined = title\n",
    "    words = combined.split()\n",
    "\n",
    "    # Remove stop words\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # duplicate first 3 words and last 1 word \n",
    "    if len(filtered_words) >= 4:\n",
    "        new_words = []\n",
    "        for x in range(3):\n",
    "            for y in range(3):\n",
    "                new_words.append(filtered_words[x]+str(y))\n",
    "        for x in range(8):\n",
    "            new_words.append(filtered_words[-1]+str(x))\n",
    "        filtered_words = new_words + filtered_words\n",
    "\n",
    "    # Join the words back into a string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply preprocessing to all product titles and include features\n",
    "preprocessed_titles = [preprocess_title(product['title'], product['featuresMap']) for products in data.values() for product in products]\n",
    "\n",
    "#print(preprocessed_titles)\n",
    "#print(len(preprocessed_titles))\n",
    "\n",
    "#for i in range(301,310):\n",
    "#    print(preprocessed_titles[i])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6203067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sets of k-shingles for the title \n",
    "def create_shingles(text, k=6):\n",
    "    \"\"\"Create a set of k-shingles from the given text.\"\"\"\n",
    "    shingles = set()\n",
    "    for i in range(len(text) - k + 1):\n",
    "        shingle = text[i:i+k]\n",
    "        shingles.add(shingle)\n",
    "    return shingles\n",
    "\n",
    "# Applying shingling to each preprocessed title\n",
    "shingles_per_title = [create_shingles(title) for title in preprocessed_titles]\n",
    "\n",
    "# check the shingles for the first title\n",
    "for i in range(1):  # Change the range as needed\n",
    "    print(f\"Title {i+1}: {preprocessed_titles[i]}\")\n",
    "    print(f\"Shingles: {shingles_per_title[i]}\\n\")\n",
    "    \n",
    "#define a hash function that convert each k-shingle for each titile into a bucket number takes only 4 bytes \n",
    "def simple_hash(shingle):\n",
    "    \"\"\"A simple hash function to convert a shingle to a bucket number.\"\"\"\n",
    "    hash_value = 0\n",
    "    for char in shingle:\n",
    "        hash_value = hash_value * 31 + ord(char)\n",
    "    return hash_value\n",
    "\n",
    "# Hashing the shingles for each title\n",
    "hashed_shingles_per_title = []\n",
    "for shingles in shingles_per_title:\n",
    "    hashed_shingles = set()\n",
    "    for shingle in shingles:\n",
    "        hashed_shingle = simple_hash(shingle)\n",
    "        hashed_shingles.add(hashed_shingle)\n",
    "    hashed_shingles_per_title.append(hashed_shingles)\n",
    "\n",
    "# Check the hashed shingles for the first title\n",
    "#for i in range(1):  # Change the range as needed\n",
    "#    print(f\"Title {i+1}: {preprocessed_titles[i]}\")\n",
    "#    print(f\"Hashed Shingles: {hashed_shingles_per_title[i]}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c32bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the characteristic matrix from the shingles \n",
    "# Identify all unique hashed shingles across all titles\n",
    "all_unique_shingles = set.union(*hashed_shingles_per_title)\n",
    "\n",
    "# Create an empty characteristic matrix\n",
    "char_matrix = np.zeros((len(all_unique_shingles), len(hashed_shingles_per_title)), dtype=int)\n",
    "\n",
    "# Create a mapping of shingle to row index\n",
    "shingle_to_index = {shingle: idx for idx, shingle in enumerate(all_unique_shingles)}\n",
    "\n",
    "# Fill the matrix\n",
    "for title_idx, shingles in enumerate(hashed_shingles_per_title):\n",
    "    for shingle in shingles:\n",
    "        row_idx = shingle_to_index[shingle]\n",
    "        char_matrix[row_idx][title_idx] = 1\n",
    "\n",
    "# Check the first few columns and rows of the matrix\n",
    "print(\"First few columns and rows of the characteristic matrix:\")\n",
    "print(char_matrix[:5, :5])  # Adjust the range as needed for rows and columns\n",
    "# Check if the matrix contains any 1s\n",
    "contains_one = np.any(char_matrix == 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f44b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now create an ALTERNATIVE WAY of minhashing \n",
    "#using random hash function to simulate the effect of random permutation \n",
    "def get_hash_functions(n, max_row):\n",
    "    \"\"\"Generate n random hash functions.\"\"\"\n",
    "    def hash_factory(a, b, p=2**33-355, m=max_row):\n",
    "        return lambda x: (a * x + b) % p % m\n",
    "\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    return [hash_factory(np.random.randint(1, max_row), np.random.randint(0, max_row)) for _ in range(n)]\n",
    "\n",
    "n = 500  # Number of hash functions\n",
    "num_titles = len(hashed_shingles_per_title)\n",
    "max_row = len(all_unique_shingles)\n",
    "\n",
    "# Generate n hash functions\n",
    "hash_functions = get_hash_functions(n, max_row)\n",
    "\n",
    "# Initialize the signature matrix with a very large integer (as infinity)\n",
    "large_integer = 2**32 - 1\n",
    "signature_matrix = np.full((n, num_titles), large_integer, dtype=int)\n",
    "\n",
    "# Fill the signature matrix\n",
    "for r, shingle in enumerate(all_unique_shingles):\n",
    "    hash_values = [h(r) for h in hash_functions]\n",
    "    for c in range(num_titles):\n",
    "        if char_matrix[shingle_to_index[shingle]][c] == 1:\n",
    "            for i in range(n):\n",
    "                signature_matrix[i][c] = min(signature_matrix[i][c], hash_values[i])\n",
    "\n",
    "# Check the first few rows and columns of the signature matrix\n",
    "print(\"First few rows and columns of the signature matrix:\")\n",
    "print(signature_matrix[:5, :5])  # Adjust the range as needed for rows and columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc619aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct candidate pairs by applying the LSH technique \n",
    "def hash_vector_to_bucket(vector, num_buckets=2**24): #A larger range can help reduce likelihood of hash collisions\n",
    "    \"\"\"Hash a vector to a bucket number.\"\"\"\n",
    "    hash_value = hash(tuple(vector))\n",
    "    return hash_value % num_buckets\n",
    "\n",
    "def apply_lsh(signature_matrix, b, r, num_buckets=2**24):\n",
    "    \"\"\"Apply LSH to find candidate pairs.\"\"\"\n",
    "    n, num_titles = signature_matrix.shape\n",
    "    assert n == b * r, \"b * r must equal n\"\n",
    "\n",
    "    candidate_pairs = set()\n",
    "    for band in range(b):\n",
    "        buckets = defaultdict(list)\n",
    "        start_row = band * r\n",
    "        end_row = start_row + r\n",
    "\n",
    "        # Hash each column vector within this band\n",
    "        for c in range(num_titles):\n",
    "            column_slice = signature_matrix[start_row:end_row, c]\n",
    "            bucket = hash_vector_to_bucket(column_slice, num_buckets)\n",
    "            buckets[bucket].append(c)\n",
    "\n",
    "        # Add pairs from the same bucket to candidate pairs\n",
    "        for bucket_items in buckets.values():\n",
    "            if len(bucket_items) > 1:\n",
    "                for pair in itertools.combinations(bucket_items, 2):\n",
    "                    candidate_pairs.add(pair)\n",
    "\n",
    "    return candidate_pairs\n",
    "\n",
    "# Choose b and r\n",
    "b = 50 # Number of bands\n",
    "r = 10 # Number of rows per band\n",
    "assert b * r == n, \"b * r must equal n\"\n",
    "\n",
    "# Calculate threshold t\n",
    "t = (1/b)**(1/r)\n",
    "print(f\"Threshold t: {t}\")\n",
    "\n",
    "# Apply LSH\n",
    "candidate_pairs = apply_lsh(signature_matrix, b, r)\n",
    "\n",
    "# Count the total number of candidate pairs\n",
    "total_candidate_pairs = len(candidate_pairs)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total number of candidate pairs: {total_candidate_pairs}\")\n",
    "\n",
    "\n",
    "# Print some of the candidate pairs\n",
    "print(\"Some candidate pairs:\")\n",
    "for i, pair in enumerate(list(candidate_pairs)[:10]):  # Display first 10 candidate pairs\n",
    "    print(f\"Pair {i+1}: {pair}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c840624",
   "metadata": {},
   "outputs": [],
   "source": [
    "##apply clustering on the candidate duplicate pairs \n",
    "\n",
    "# Count the number of products for each model ID\n",
    "model_id_counts = {model_id: len(products) for model_id, products in data.items()}\n",
    "# Calculate the total number of actual duplicate pairs\n",
    "total_actual_duplicates = sum(count * (count - 1) // 2 for count in model_id_counts.values() if count > 1)\n",
    "print(f\"Total actual duplicates: {total_actual_duplicates}\")\n",
    "\n",
    "def is_duplicate(pair_indices, index_to_modelid):\n",
    "    model_id1 = index_to_modelid[pair_indices[0]]\n",
    "    model_id2 = index_to_modelid[pair_indices[1]]\n",
    "    return model_id1 == model_id2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8a6938",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_indice = list(candidate_pairs)[14]\n",
    "print(pair_indice)\n",
    "dup = is_duplicate(pair_indice, index_to_modelid)\n",
    "print(dup)\n",
    "print(index_to_modelid[pair_indice[0]])\n",
    "print(index_to_modelid[pair_indice[1]])\n",
    "print(preprocessed_titles[pair_indice[0]])\n",
    "print(preprocessed_titles[pair_indice[1]])\n",
    "\n",
    "# minhash_obj = minhashes[pair_indice[0]]\n",
    "\n",
    "# # To view the hash values\n",
    "# print(minhash_obj.hashvalues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e137e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_titles = []\n",
    "candidate_indices = []\n",
    "for i in candidate_pairs:\n",
    "    if i[0] not in candidate_indices:\n",
    "        candidate_indices.append(i[0])\n",
    "        candidate_titles.append(preprocessed_titles[i[0]])\n",
    "    if i[1] not in candidate_indices:\n",
    "        candidate_indices.append(i[1])\n",
    "        candidate_titles.append(preprocessed_titles[i[1]])\n",
    "\n",
    "print(len(candidate_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d330ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "# Create a TfidfVectorizer with q-gram (bigram) configuration\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(2, 2))  # This will create bigrams\n",
    "tfidf_matrix = vectorizer.fit_transform(candidate_titles)\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6823d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clustering(similarity_matrix, candidate_indices):\n",
    "\n",
    "    # Uses the average of the distances of each observation of the two sets.\n",
    "    Z = linkage(similarity_matrix, method='average', metric='cosine')\n",
    "\n",
    "    max_dist = 0.6 # can adjust this threshold \n",
    "    clusters = fcluster(Z, max_dist, criterion='inconsistent')\n",
    "\n",
    "    cluster_dict = {}\n",
    "    for idx, cluster_id in enumerate(clusters):\n",
    "        if cluster_id not in cluster_dict:\n",
    "            cluster_dict[cluster_id] = []\n",
    "        cluster_dict[cluster_id].append(candidate_indices[idx])\n",
    "\n",
    "    # Filter clusters with more than one title (potential duplicates)\n",
    "    duplicate_clusters = {k: v for k, v in cluster_dict.items() if len(v) > 1}\n",
    "    return duplicate_clusters\n",
    "\n",
    "\n",
    "duplicate_clusters = clustering(similarity_matrix, candidate_indices)\n",
    "print(f\"Number of duplicate clusters: {len(duplicate_clusters)}\")\n",
    "#print(duplicate_clusters)\n",
    "\n",
    "\n",
    "def all_true_duplicates(candidate_indices, index_to_modelid):\n",
    "    true_duplicates = []\n",
    "    for pair in itertools.combinations(candidate_indices, 2):\n",
    "        if is_duplicate(pair, index_to_modelid):\n",
    "            true_duplicates.append(pair)\n",
    "    return true_duplicates\n",
    "\n",
    "\n",
    "false_negatives = 0\n",
    "for model_id, products in data.items():\n",
    "    if len(products) > 1:\n",
    "        for pair in itertools.combinations(products, 2):\n",
    "            pair_indices = (product_titles.index(pair[0]['title']), product_titles.index(pair[1]['title']))\n",
    "            if is_duplicate(pair_indices, index_to_modelid):\n",
    "                if not any(pair_indices[0] in cluster and pair_indices[1] in cluster for cluster in duplicate_clusters.values()):\n",
    "                    false_negatives += 1\n",
    "\n",
    "true_positives, false_positives = 0,0\n",
    "\n",
    "for cluster in duplicate_clusters.values():\n",
    "    for pair in itertools.combinations(cluster, 2):\n",
    "        if is_duplicate(pair, index_to_modelid):\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "            \n",
    "\n",
    "        \n",
    "PC = true_positives / total_actual_duplicates\n",
    "PQ = true_positives / total_candidate_pairs\n",
    "f1star = 2* PC * PQ / (PC+PQ)\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(f\"FN:{false_negatives:.3f}\")\n",
    "print(f\"TP:{true_positives:.3f}\")  \n",
    "print(f\"Candidate Pairs:{total_candidate_pairs}\")\n",
    "print(f\"Pair completeness:{PC:.3f}\")\n",
    "print(f\"Pair Quality:{PQ:.3f}\")\n",
    "print(f\"F1star:{f1star:.3f}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f601b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "# Create a TfidfVectorizer with q-gram (bigram) configuration\n",
    "tfidf_matrix_full = vectorizer.fit_transform(preprocessed_titles)\n",
    "print(tfidf_matrix_full.shape)\n",
    "\n",
    "def estimate_jaccard_similarity(pair_indices, minhashes, num_perm):\n",
    "    minhash1 = minhashes[pair_indices[0]]\n",
    "    minhash2 = minhashes[pair_indices[1]]\n",
    "    count = sum(1 for i in range(num_perm) if minhash1.hashvalues[i] == minhash2.hashvalues[i]) / num_perm\n",
    "    return count\n",
    "\n",
    "# Number of bootstrap iterations\n",
    "n_bootstrap = 5 \n",
    "\n",
    "# Bootstrapping and evaluation\n",
    "precision_scores, recall_scores, f1_scores = [], [], []\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "   true_positives, false_positives = 0, 0\n",
    "   for pair_indices in candidate_pairs:\n",
    "      #  if estimate_jaccard_similarity(pair_indices, minhashes, num_perm) > 0.6:\n",
    "         if cosine_similarity(tfidf_matrix_full[pair_indices[0]], tfidf_matrix_full[pair_indices[1]]) > 0.65:\n",
    "\n",
    "            if is_duplicate(pair_indices, index_to_modelid):\n",
    "               true_positives += 1\n",
    "            else:\n",
    "               false_positives += 1\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "   precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "   recall = true_positives / total_actual_duplicates if total_actual_duplicates > 0 else 0\n",
    "   f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Store the scores\n",
    "   precision_scores.append(precision)\n",
    "   recall_scores.append(recall)\n",
    "   f1_scores.append(f1)\n",
    "\n",
    "    # Calculate average metrics over all bootstraps\n",
    "   avg_precision = sum(precision_scores) / n_bootstrap\n",
    "   avg_recall = sum(recall_scores) / n_bootstrap\n",
    "   avg_f1 = sum(f1_scores) / n_bootstrap\n",
    "\n",
    "   avg_precision, avg_recall, avg_f1\n",
    "    \n",
    "print(f\"FN:{false_negatives:.3f}\")\n",
    "print(f\"TP:{true_positives:.3f}\")  \n",
    "print(f\"Candidate Pairs:{total_candidate_pairs}\")\n",
    "print(f\"Pair completeness:{PC:.3f}\")\n",
    "print(f\"Pair Quality:{PQ:.3f}\")\n",
    "print(f\"F1star:{f1star:.3f}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01979f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62937b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
